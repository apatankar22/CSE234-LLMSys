{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E-mNhUjQuxNM"
      },
      "outputs": [],
      "source": [
        "#amogh patankar\n",
        "# %pip install torch torchvision torchaudio\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N9lmLw8cuxNN"
      },
      "outputs": [],
      "source": [
        "def is_cuda():\n",
        "    return triton.runtime.driver.active.get_current_target().backend == \"cuda\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eUMlpjFJuxNO"
      },
      "outputs": [],
      "source": [
        "def is_hip_mi200():\n",
        "    target = triton.runtime.driver.active.get_current_target()\n",
        "    return target.backend == 'hip' and target.arch == 'gfx90a'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lBNGYaejuxNO"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "PA2 Part 1: MatMul+Relu+Add Fused Optimization.\n",
        "The kernel uses several optimization techniques:\n",
        "\n",
        "    1. Shared memory tiling.\n",
        "    2. Register tiling.\n",
        "    3. Cooperative fetching.\n",
        "    4. Operator Fusion\n",
        "    5. Write cache / epilogue fusion.\n",
        "\n",
        "Fill in the missing parts (marked with TODO).\n",
        "\"\"\"\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Tiling parameters - You will need to change these to achieve better results.\n",
        "# -----------------------------------------------------------------------------\n",
        "BLOCK_M = 128  # Tile size in the M dimension.\n",
        "BLOCK_N = 256 # Tile size in the N dimension.\n",
        "BLOCK_K = 16 # Tile size in the K dimension.\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Triton Kernel: Matrix Multiplication + ReLU + Add\n",
        "#\n",
        "# The kernel uses:\n",
        "#   Step 1: Tile assignment (each kernel computes a tile of C)\n",
        "#   Step 2: Shared memory tiling + Cooperative Fetching: Load tiles of A and B.\n",
        "#   Step 3: Register tiling: Use a register accumulator.\n",
        "#   Step 4: Add and ReLU fusion\n",
        "#   Step 5: Write cache/Epilogue: Write the final tile back to global memory.\n",
        "# -----------------------------------------------------------------------------\n",
        "@triton.jit\n",
        "def matmul_add_relu_kernel_fp16(\n",
        "    a_ptr, b_ptr, c_ptr, d_ptr,\n",
        "    M: tl.constexpr, N: tl.constexpr, K: tl.constexpr,\n",
        "    stride_am: tl.constexpr, stride_ak: tl.constexpr,\n",
        "    stride_bk: tl.constexpr, stride_bn: tl.constexpr,\n",
        "    stride_cm: tl.constexpr, stride_cn: tl.constexpr,\n",
        "    stride_dm: tl.constexpr, stride_dn: tl.constexpr,\n",
        "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
        "):\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 1: Tile: Assignment\n",
        "    #\n",
        "    # Each kernel instance is mapped to a tile in the output matrix C.\n",
        "    # Compute the starting indices (m_start, n_start) for this tile.\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: Compute the tile indices using program_id(0) for M and program_id(1) for N.\n",
        "\n",
        "    pid_m = tl.program_id(0) # row block\n",
        "    pid_n = tl.program_id(1) # column block\n",
        "\n",
        "    m_init = pid_m * BLOCK_M\n",
        "    n_init = pid_n * BLOCK_N\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 2: Register Tiling\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: Initialize the accumulator \"acc\" with zeros (dtype: float16).\n",
        "\n",
        "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float16)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 3: Shared Memory Tiling & Cooperative Fetching.\n",
        "    # Compute pointers to the sub-tiles of A and B that are needed to compute\n",
        "    # the current C tile. The offsets here serve to load BLOCK_SIZE_M x BLOCK_SIZE_K\n",
        "    # and BLOCK_SIZE_K x BLOCK_SIZE_N blocks from A and B respectively.\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: Finish code below\n",
        "\n",
        "    offs_am = m_init + tl.arange(0, BLOCK_M)\n",
        "    offs_bn = n_init + tl.arange(0, BLOCK_N)\n",
        "    offs_k = tl.arange(0, BLOCK_K)\n",
        "\n",
        "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
        "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
        "\n",
        "    m1 = offs_k[None, :]\n",
        "    m2 = offs_k[:, None]\n",
        "\n",
        "    for k in range(tl.cdiv(K, BLOCK_K)):\n",
        "        a = tl.load(a_ptrs, mask = m1 < K - k * BLOCK_K, other = 0.0)\n",
        "        b = tl.load(b_ptrs, mask = m2 < K - k * BLOCK_K, other = 0.0)\n",
        "\n",
        "        acc += tl.dot(a, b, out_dtype=tl.float16)\n",
        "        a_ptrs += (BLOCK_K * stride_ak)\n",
        "        b_ptrs += (BLOCK_K * stride_bk)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 4: Apply ReLU and Add C to the accumulator\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: Finish code below\n",
        "\n",
        "    temp_m1 = offs_am[:, None]\n",
        "    temp_m2 = offs_bn[None, :]\n",
        "\n",
        "    c_ptrs = c_ptr + (temp_m1 * stride_cm + temp_m2 * stride_cn)\n",
        "    c = tl.load(c_ptrs, mask = (temp_m1 < M) & (temp_m2 < N), other = 0.0)\n",
        "    acc += c\n",
        "    acc = tl.maximum(acc, 0)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 5: Write Cache / Epilogue Fusion: Write the computed tile to D.\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: Finish code below\n",
        "\n",
        "    d_ptrs = d_ptr + (temp_m1 * stride_dm + temp_m2 * stride_dn)\n",
        "    tl.store(d_ptrs, acc, mask=(temp_m1 < M) & (temp_m2 < N))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u16sz-IUuxNP"
      },
      "outputs": [],
      "source": [
        "def matmul_add_relu_fp16(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes Output = ReLU(A @ B + C) using fp16 precision for maximum throughput.\n",
        "    \"\"\"\n",
        "    M, K = a.shape\n",
        "    K2, N = b.shape\n",
        "    assert K == K2, \"Incompatible dimensions\"\n",
        "\n",
        "    d = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
        "    # Create launch grid\n",
        "    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n",
        "\n",
        "    matmul_add_relu_kernel_fp16[grid](\n",
        "        a, b, c, d,\n",
        "        M, N, K,\n",
        "        a.stride(0), a.stride(1),\n",
        "        b.stride(0), b.stride(1),\n",
        "        c.stride(0), c.stride(1),\n",
        "        d.stride(0), d.stride(1),\n",
        "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K\n",
        "    )\n",
        "    return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AJ7LlTPawPqB"
      },
      "outputs": [],
      "source": [
        "# Reference implementation using PyTorch\n",
        "def reference_matmul_add_relu(A, B, C):\n",
        "    result = torch.matmul(A, B).add(C).relu_()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4J5ZBpOuxNP",
        "outputId": "afdd27d1-b36f-4953-8e73-5d20ab86bc6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "triton_output_with_fp16_inputs=tensor([[ 0.0000,  6.1250,  0.0000,  ..., 10.0625,  0.0000,  0.0000],\n",
            "        [ 7.9102, 15.6328, 26.6094,  ..., 11.4609,  5.3750, 18.6250],\n",
            "        [ 2.7246,  0.0000,  0.0000,  ...,  0.0000, 26.0781,  0.0000],\n",
            "        ...,\n",
            "        [ 0.4448, 75.1875,  0.0000,  ..., 26.2812,  0.0000,  0.0000],\n",
            "        [ 6.9492,  1.1230,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [27.6094, 26.9531, 22.9219,  ..., 13.5391,  6.0508, 21.6250]],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "torch_output_with_fp16_inputs=tensor([[ 0.0000,  6.1289,  0.0000,  ..., 10.0391,  0.0000,  0.0000],\n",
            "        [ 7.9102, 15.6328, 26.6250,  ..., 11.4531,  5.3945, 18.6562],\n",
            "        [ 2.7266,  0.0000,  0.0000,  ...,  0.0000, 26.1250,  0.0000],\n",
            "        ...,\n",
            "        [ 0.4316, 75.2500,  0.0000,  ..., 26.2812,  0.0000,  0.0000],\n",
            "        [ 6.9570,  1.1260,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [27.6406, 26.9531, 22.9375,  ..., 13.5625,  6.0391, 21.6406]],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "✅ Triton and Torch match\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Accuracy Tests\n",
        "# -----------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(0)\n",
        "    a = torch.randn((512, 512), device=torch.device(\"cuda\"), dtype=torch.float16)\n",
        "    b = torch.randn((512, 512), device=torch.device(\"cuda\"), dtype=torch.float16)\n",
        "    c = torch.randn((512, 512), device=torch.device(\"cuda\"), dtype=torch.float16)\n",
        "    triton_output = matmul_add_relu_fp16(a, b, c)\n",
        "    torch_output = reference_matmul_add_relu(a, b, c)\n",
        "    print(f\"triton_output_with_fp16_inputs={triton_output}\")\n",
        "    print(f\"torch_output_with_fp16_inputs={torch_output}\")\n",
        "    rtol = 1e-2 if is_hip_mi200() else 0.032\n",
        "    if torch.allclose(triton_output, torch_output, atol=0.15, rtol=rtol):\n",
        "        print(\"✅ Triton and Torch match\")\n",
        "    else:\n",
        "        diff = triton_output - torch_output\n",
        "        abs_diff = torch.abs(diff)\n",
        "        max_abs_diff = torch.max(abs_diff)\n",
        "        print(f\"❌ Triton and Torch differ: {max_abs_diff=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kj_dGOlazQJY",
        "outputId": "fed94a54-21f1-4049-c0c6-d330ab6e3232"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 0.67 ms\n",
            "PyTorch implementation: 0.88 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 1.31x\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Performance Benchmark\n",
        "# IMPORTANT: DO NOT CHANGE THIS CODE.\n",
        "# THIS IS THE EXACT CODE THAT WILL BE USED TO GRADE YOUR IMPLEMENTATION.\n",
        "# ANY CHANGES TO THIS CODE (INCLUDING DIMENSIONS, REPEATS, etc.)\n",
        "# WILL CAUSE YOU TO HAVE DIFFERENT SPEEDUP RESULTS.\n",
        "# -----------------------------------------------------------------------------\n",
        "M = 2048\n",
        "K = 2048\n",
        "N = 2048\n",
        "\n",
        "# KEEP THESE MATRICES IN FP16. FP32 WILL NOT PROVIDE ACCURATE RESULTS\n",
        "A = torch.randn((M, K), device=\"cuda\", dtype=torch.float16)\n",
        "B = torch.randn((K, N), device=\"cuda\", dtype=torch.float16)\n",
        "C = torch.randn((M, N), device=\"cuda\", dtype=torch.float16)\n",
        "\n",
        "# warmup\n",
        "_ = matmul_add_relu_fp16(A, B, C)\n",
        "_ = reference_matmul_add_relu(A, B, C)\n",
        "\n",
        "REPEATS = 5000\n",
        "\n",
        "# time your implementation\n",
        "print(\"Triton implementation\")\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "for _ in range(REPEATS):\n",
        "    _ = matmul_add_relu_fp16(A, B, C)\n",
        "torch.cuda.synchronize()\n",
        "triton_time = (time.perf_counter() - start) / REPEATS\n",
        "\n",
        "# time pytorch\n",
        "print(\"PyTorch implementation\")\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "for _ in range(REPEATS):\n",
        "    _ = reference_matmul_add_relu(A, B, C)\n",
        "torch.cuda.synchronize()\n",
        "torch_time = (time.perf_counter() - start) / REPEATS\n",
        "\n",
        "print(f\"Performance comparison for matrix multiplication ({M}x{K} @ {K}x{N}):\")\n",
        "print(f\"Triton implementation: {triton_time*1000:.2f} ms\")\n",
        "print(f\"PyTorch implementation: {torch_time*1000:.2f} ms\")\n",
        "\n",
        "print(f\"\\nSpeedup of Triton vs PyTorch: {torch_time/triton_time:.2f}x\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9Hdpxic0tq6",
        "outputId": "eb32c6d6-ba43-44c9-89fe-37a6da321f46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "M: 32, N: 32, K: 16, Time: 0.661548867199997ms, Speedup: 1.343394244119125x\n",
            "M: 32, N: 32, K: 32, Time: 0.6700959573999966ms, Speedup: 1.3512875112892169x\n",
            "M: 32, N: 32, K: 64, Time: 0.6770457776000058ms, Speedup: 1.3613741771897523x\n",
            "M: 32, N: 32, K: 128, Time: 0.6841005047999942ms, Speedup: 1.3459846325785243x\n",
            "M: 32, N: 64, K: 16, Time: 0.6881618090000075ms, Speedup: 1.361221587639705x\n",
            "M: 32, N: 64, K: 32, Time: 0.696216911199997ms, Speedup: 1.3484656688126806x\n",
            "M: 32, N: 64, K: 64, Time: 0.7036665545999994ms, Speedup: 1.3566787734322265x\n",
            "M: 32, N: 64, K: 128, Time: 0.7104441606000024ms, Speedup: 1.3786858871115022x\n",
            "M: 32, N: 128, K: 16, Time: 0.7189502887999993ms, Speedup: 1.3841294188238666x\n",
            "M: 32, N: 128, K: 32, Time: 0.7260297388000027ms, Speedup: 1.4060962751846955x\n",
            "M: 32, N: 128, K: 64, Time: 0.7357112255999937ms, Speedup: 1.4063134743611116x\n",
            "M: 32, N: 128, K: 128, Time: 0.7419417786000054ms, Speedup: 1.394786855045009x\n",
            "M: 32, N: 256, K: 16, Time: 0.733997811200004ms, Speedup: 1.381749355276544x\n",
            "M: 32, N: 256, K: 32, Time: 0.7259674580000024ms, Speedup: 1.385996583334442x\n",
            "M: 32, N: 256, K: 64, Time: 0.7222583217999954ms, Speedup: 1.3741395432129573x\n",
            "M: 32, N: 256, K: 128, Time: 0.7200445314000035ms, Speedup: 1.3777743577485593x\n",
            "M: 32, N: 512, K: 16, Time: 0.7193402417999891ms, Speedup: 1.3704213560098446x\n",
            "M: 32, N: 512, K: 32, Time: 0.7183920752000063ms, Speedup: 1.3767525468939015x\n",
            "M: 32, N: 512, K: 64, Time: 0.719663459200001ms, Speedup: 1.375946725016102x\n",
            "M: 32, N: 512, K: 128, Time: 0.7205133411999896ms, Speedup: 1.3849746456298055x\n",
            "M: 64, N: 32, K: 16, Time: 0.7220100988000013ms, Speedup: 1.3832033644125457x\n",
            "M: 64, N: 32, K: 32, Time: 0.7232064792000074ms, Speedup: 1.392649679679434x\n",
            "M: 64, N: 32, K: 64, Time: 0.7237302034000095ms, Speedup: 1.3856223591179218x\n",
            "M: 64, N: 32, K: 128, Time: 0.7233264535999979ms, Speedup: 1.3910144530071449x\n",
            "M: 64, N: 64, K: 16, Time: 0.7230318686000146ms, Speedup: 1.3847135019630283x\n",
            "M: 64, N: 64, K: 32, Time: 0.722450885399985ms, Speedup: 1.3887376469121804x\n",
            "M: 64, N: 64, K: 64, Time: 0.7220594178000056ms, Speedup: 1.3822157517741096x\n",
            "M: 64, N: 64, K: 128, Time: 0.7218234471999949ms, Speedup: 1.3866100275386264x\n",
            "M: 64, N: 128, K: 16, Time: 0.722416506400009ms, Speedup: 1.3791562675733156x\n",
            "M: 64, N: 128, K: 32, Time: 0.7216512526000087ms, Speedup: 1.386852480327812x\n",
            "M: 64, N: 128, K: 64, Time: 0.7219370362000063ms, Speedup: 1.3806841763467343x\n",
            "M: 64, N: 128, K: 128, Time: 0.7214259732000072ms, Speedup: 1.3850222214316987x\n",
            "M: 64, N: 256, K: 16, Time: 0.7220872483999983ms, Speedup: 1.378450444881166x\n",
            "M: 64, N: 256, K: 32, Time: 0.7214519694000047ms, Speedup: 1.3836111890167546x\n",
            "M: 64, N: 256, K: 64, Time: 0.7219770403999973ms, Speedup: 1.3782498216961314x\n",
            "M: 64, N: 256, K: 128, Time: 0.721260630200004ms, Speedup: 1.3840008243943396x\n",
            "M: 64, N: 512, K: 16, Time: 0.7211782086000085ms, Speedup: 1.379526762922186x\n",
            "M: 64, N: 512, K: 32, Time: 0.7211961008000117ms, Speedup: 1.3834544231357266x\n",
            "M: 64, N: 512, K: 64, Time: 0.7213947239999925ms, Speedup: 1.3779995136199683x\n",
            "M: 64, N: 512, K: 128, Time: 0.7211393302000033ms, Speedup: 1.382347282214565x\n",
            "M: 128, N: 32, K: 16, Time: 0.7216263975999937ms, Speedup: 1.3773972968640873x\n",
            "M: 128, N: 32, K: 32, Time: 0.7211729611999999ms, Speedup: 1.3821145675531965x\n",
            "M: 128, N: 32, K: 64, Time: 0.7206304607999982ms, Speedup: 1.3793268573417743x\n",
            "M: 128, N: 32, K: 128, Time: 0.7206023977999848ms, Speedup: 1.3835450090699388x\n",
            "M: 128, N: 64, K: 16, Time: 0.7208558002000018ms, Speedup: 1.3791882325482663x\n",
            "M: 128, N: 64, K: 32, Time: 0.7207232561999944ms, Speedup: 1.3846654482356449x\n",
            "M: 128, N: 64, K: 64, Time: 0.7210319675999927ms, Speedup: 1.3796092363436827x\n",
            "M: 128, N: 64, K: 128, Time: 0.7209984917999919ms, Speedup: 1.382380107774874x\n",
            "M: 128, N: 128, K: 16, Time: 0.7211580616000219ms, Speedup: 1.3782646608633171x\n",
            "M: 128, N: 128, K: 32, Time: 0.7210940594000022ms, Speedup: 1.3837397518296326x\n",
            "M: 128, N: 128, K: 64, Time: 0.7218211139999994ms, Speedup: 1.3776060169389979x\n",
            "M: 128, N: 128, K: 128, Time: 0.7210595809999859ms, Speedup: 1.3848174063718943x\n",
            "M: 128, N: 256, K: 16, Time: 0.7210904961999859ms, Speedup: 1.3780243797921439x\n",
            "M: 128, N: 256, K: 32, Time: 0.7204425648000097ms, Speedup: 1.3850556926449515x\n",
            "M: 128, N: 256, K: 64, Time: 0.7208350173999861ms, Speedup: 1.3788006459299262x\n",
            "M: 128, N: 256, K: 128, Time: 0.7210734929999943ms, Speedup: 1.383210184929086x\n",
            "M: 128, N: 512, K: 16, Time: 0.7216233754000086ms, Speedup: 1.3773385814297596x\n",
            "M: 128, N: 512, K: 32, Time: 0.7214146194000023ms, Speedup: 1.383153856002943x\n",
            "M: 128, N: 512, K: 64, Time: 0.721430878000001ms, Speedup: 1.3778299076352958x\n",
            "M: 128, N: 512, K: 128, Time: 0.7210334547999991ms, Speedup: 1.383426866200952x\n",
            "M: 256, N: 32, K: 16, Time: 0.7208790131999877ms, Speedup: 1.3781593490839783x\n",
            "M: 256, N: 32, K: 32, Time: 0.7205233077999992ms, Speedup: 1.3845286502194831x\n",
            "M: 256, N: 32, K: 64, Time: 0.7209851842000035ms, Speedup: 1.3772927673983189x\n",
            "M: 256, N: 32, K: 128, Time: 0.7209297498000069ms, Speedup: 1.3842953440010837x\n",
            "M: 256, N: 64, K: 16, Time: 0.7205121191999978ms, Speedup: 1.3797609135343905x\n",
            "M: 256, N: 64, K: 32, Time: 0.7212144906000049ms, Speedup: 1.3846101347315274x\n",
            "M: 256, N: 64, K: 64, Time: 0.7216811873999859ms, Speedup: 1.3786098437516363x\n",
            "M: 256, N: 64, K: 128, Time: 0.7213329211999963ms, Speedup: 1.38551341083586x\n",
            "M: 256, N: 128, K: 16, Time: 0.7215046512000072ms, Speedup: 1.3796566851570542x\n",
            "M: 256, N: 128, K: 32, Time: 0.7215840994000018ms, Speedup: 1.3855347342483244x\n",
            "M: 256, N: 128, K: 64, Time: 0.7215885258000072ms, Speedup: 1.3805669311267772x\n",
            "M: 256, N: 128, K: 128, Time: 0.7221250646000044ms, Speedup: 1.3860149484693585x\n",
            "M: 256, N: 256, K: 16, Time: 0.7216863037999929ms, Speedup: 1.3811144564498072x\n",
            "M: 256, N: 256, K: 32, Time: 0.7206788162000066ms, Speedup: 1.387284731458694x\n",
            "M: 256, N: 256, K: 64, Time: 0.7219764303999909ms, Speedup: 1.3807693933272844x\n",
            "M: 256, N: 256, K: 128, Time: 0.7225271181999915ms, Speedup: 1.3844318617299598x\n",
            "M: 256, N: 512, K: 16, Time: 0.7219525230000046ms, Speedup: 1.3815510234043147x\n",
            "M: 256, N: 512, K: 32, Time: 0.721628107000015ms, Speedup: 1.3865354551662163x\n",
            "M: 256, N: 512, K: 64, Time: 0.721332354800029ms, Speedup: 1.3826316015957854x\n",
            "M: 256, N: 512, K: 128, Time: 0.7220603144000051ms, Speedup: 1.386380998700669x\n",
            "M: 512, N: 32, K: 16, Time: 0.7224699637999947ms, Speedup: 1.3800820645271645x\n",
            "M: 512, N: 32, K: 32, Time: 0.7217058383999756ms, Speedup: 1.3867629387880027x\n",
            "M: 512, N: 32, K: 64, Time: 0.7219059042000027ms, Speedup: 1.380737651819828x\n",
            "M: 512, N: 32, K: 128, Time: 0.7220386689999941ms, Speedup: 1.3872215359147713x\n",
            "M: 512, N: 64, K: 16, Time: 0.7223456048000116ms, Speedup: 1.3812383423807537x\n",
            "M: 512, N: 64, K: 32, Time: 0.7217961767999896ms, Speedup: 1.387300651049963x\n",
            "M: 512, N: 64, K: 64, Time: 0.722125989999995ms, Speedup: 1.3812339181975337x\n",
            "M: 512, N: 64, K: 128, Time: 0.7221712489999845ms, Speedup: 1.3874817506062362x\n",
            "M: 512, N: 128, K: 16, Time: 0.7220356014000117ms, Speedup: 1.3831557500261318x\n",
            "M: 512, N: 128, K: 32, Time: 0.7223454146000222ms, Speedup: 1.386234714806728x\n",
            "M: 512, N: 128, K: 64, Time: 0.7219453663999957ms, Speedup: 1.381071235586524x\n",
            "M: 512, N: 128, K: 128, Time: 0.7217089178000151ms, Speedup: 1.3878132863497739x\n",
            "M: 512, N: 256, K: 16, Time: 0.721979077800006ms, Speedup: 1.3837937977986046x\n",
            "M: 512, N: 256, K: 32, Time: 0.7220057101999827ms, Speedup: 1.3885754462001616x\n",
            "M: 512, N: 256, K: 64, Time: 0.7224095777999991ms, Speedup: 1.3811366710260504x\n",
            "M: 512, N: 256, K: 128, Time: 0.7224006420000023ms, Speedup: 1.3872442776705263x\n",
            "M: 512, N: 512, K: 16, Time: 0.7223514373999933ms, Speedup: 1.3817705378875322x\n",
            "M: 512, N: 512, K: 32, Time: 0.7224719895999897ms, Speedup: 1.3871509744687576x\n",
            "M: 512, N: 512, K: 64, Time: 0.7219386807999854ms, Speedup: 1.3815164765722023x\n",
            "M: 512, N: 512, K: 128, Time: 0.721958946399991ms, Speedup: 1.3872673727421934x\n",
            "Best combo: [32, 128, 64], Best speedup: 1.4063134743611116x, Best time: 0.7357112255999937ms\n"
          ]
        }
      ],
      "source": [
        "# Write your grid search here.\n",
        "\n",
        "best_combo = list()\n",
        "best_speedup = -1\n",
        "best_time = -1\n",
        "\n",
        "for block_size_m in [32, 64, 128, 256, 512]:\n",
        "  for block_size_n in [32, 64, 128, 256, 512]:\n",
        "    for block_size_k in [16, 32, 64, 128]:\n",
        "      A = torch.randn((M, K), device=\"cuda\", dtype=torch.float16)\n",
        "      B = torch.randn((K, N), device=\"cuda\", dtype=torch.float16)\n",
        "      C = torch.randn((M, N), device=\"cuda\", dtype=torch.float16)\n",
        "\n",
        "      # warmup\n",
        "      _ = matmul_add_relu_fp16(A, B, C)\n",
        "      _ = reference_matmul_add_relu(A, B, C)\n",
        "\n",
        "      REPEATS = 5000\n",
        "      torch.cuda.synchronize()\n",
        "      start = time.perf_counter()\n",
        "\n",
        "      for _ in range(REPEATS):\n",
        "          _ = matmul_add_relu_fp16(A, B, C)\n",
        "\n",
        "      torch.cuda.synchronize()\n",
        "      triton_time = (time.perf_counter() - start) / REPEATS\n",
        "\n",
        "      torch.cuda.synchronize()\n",
        "      start = time.perf_counter()\n",
        "\n",
        "      for _ in range(REPEATS):\n",
        "          _ = reference_matmul_add_relu(A, B, C)\n",
        "\n",
        "      torch.cuda.synchronize()\n",
        "      torch_time = (time.perf_counter() - start) / REPEATS\n",
        "\n",
        "      print(\"M: {}, N: {}, K: {}, Time: {}ms, Speedup: {}x\".format(block_size_m, block_size_n, block_size_k, triton_time*1000, torch_time/triton_time))\n",
        "      if torch_time/triton_time > best_speedup:\n",
        "        best_speedup = torch_time/triton_time\n",
        "        best_combo = [block_size_m, block_size_n, block_size_k]\n",
        "        best_time = triton_time\n",
        "\n",
        "print(\"Best combo: {}, Best speedup: {}x, Best time: {}ms\".format(best_combo, best_speedup, best_time*1000))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
