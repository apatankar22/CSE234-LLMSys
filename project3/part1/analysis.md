MoE Analysis

To benchmark, I ran the following command: mpirun -n 4 python3 avp_benchmark.py. From this, I generated a plot that showed the SimpleMoE, MoE with Tensor Parallelism, and MoE with Expert Parallelism over time. In this graph, I noted a few patterns, which are described below.

SimpleMoE shows an almost linear ascent for its forward pass; that is, he average time taken increases linearly as compared to the increasing batch sizes. This reflects the lack of parallelism that I implement in Tensor Parallelism and Expert Parallelism. When implementing Tensor Parallelism (nicknamed MoE_TP), the model initially improves its performance slightly between batch sizes of ~10 to 30; then, it begins to take longer as batch size increases. The reasoning for this may be the time it takes to complete the communication, i.e. the overhead, and the time taken to execute increases with increased batch size. This showed that the parallelism strategy worked, as the time was approximately 50% of the original non-parallelized MoE for all batch sizes. Finally, I tried expert parallelism (nicknamed MoE_EP), and this strategy showed a relatively flat, or linear trend. As batch size increased, the time taken increased in a fairly consistent manner. It showed the lowest time for batch sizes around of 15-40 and 80 onwards. Expert parallelism distributes all compute effectively, thereby reducing the overhead for each process and by extension reduced time. 