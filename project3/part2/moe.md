Writeup: Advantages of Mixture of Experts (MoE) Models

There are many advantages to MoE models, such as DeepSeek-V3. Primarily, they are surrounding parameter efficiency, computational cost, memory, flexibility, and scaling. Deepseek-V3 has a high parameter count, as it has a large number of experts, but they only use a miniscule number of experts for inference tasks. This parameter efficiency helps MoE models to have a high model capacity when they are constrained by compute. Moreover, the inner workings (MLP) of DeepSeek's model is 1/9th the size of a regular MLP, leading to greatly reudced FLOPs. Finally, because only the active experts work during inference, their approach reduces memory usage by a significant amount, and scaling the model through more experts can be dowe without throwing money at the problem.